Karl-Learning-GPT â€” Dev Handbook


Current Folder / File Tree

karl_learning_gpt/
â”œâ”€ .env
â”œâ”€ app.py                     â† FastAPI back-end entry
â”œâ”€ prompts.py                 â† System prompt template (JSON-only spec)
â”œâ”€ learner_profile.py         â† Pydantic models: LearnerProfile, LearnerSnapshot
â”œâ”€ utils/
â”‚  â”œâ”€ readaloud.py            â† Whisper transcription + fluency scorer
â”‚  â”œâ”€ mood.py                 â† (stub) returns 0.0 for now
â”‚  â””â”€ comic.py                â† (stub) placeholder for reward comics
â”œâ”€ passages.json              â† â‰ˆ 5 public-domain 40-60-word passages (legacy)
â”œâ”€ store_profile.py           â† Script that embeddings â†’ Pinecone
â”œâ”€ requirements.txt           â† freeze once happy
â”‚
â”œâ”€ frontend/
â”‚  â”œâ”€ vite.config.js          â† Vite + proxy â€œ/api â†’ :8000â€ rewrite
â”‚  â”œâ”€ index.html
â”‚  â”œâ”€ src/
â”‚  â”‚  â”œâ”€ App.jsx
â”‚  â”‚  â”œâ”€ AudioRecorder.jsx
â”‚  â”‚  â”œâ”€ WebcamSnap.jsx
â”‚  â”‚  â””â”€ ActivityBox.jsx
â”‚  â””â”€ package.json
â””â”€ venv/  (ignored)




Completed Milestones
| Area                     | Whatâ€™s done                                                                                                                                                         |
| ------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Environment**          | Python 3.13 venv, Node â‰¥ 18, React+Vite dev server, FastAPI+Uvicorn.                                                                                                |
| **Secrets**              | `.env` with `OPENAI_API_KEY`, Pinecone keys, Whisper uses same key.                                                                                                 |
| **Vector store**         | Pinecone index **karl-profile** (1 k dim, text-embedding-3-small). `store_profile.py` uploads Karlâ€™s snapshot.                                                      |
| **Assistant**            | `client.beta.assistants.create()` with GPT-4o-mini, Code Interpreter tool, JSON-only instructions.                                                                  |
| **Session flow**         | `/start_session` â†’ thread; `/submit_audio`, `/submit_mood`, `/next_activity` routes wired; React proxy `/api/*`.                                                    |
| **Fluency scoring**      | `readaloud.score(audio, passage_text)` â†’ transcription, WPM, accuracy.                                                                                              |
| **Front-end UI**         | Buttons: **Start Adventure**, **Start/Stop reading**, **Mood check**, **Next Task**; Activity card renders `title`, `story_prompt`, `steps`, **read\_aloud** block. |
| **Audio/Webcam capture** | Native `MediaRecorder` (WebM) and `react-webcam`; uploads succeed (200 OK).                                                                                         |
| **Back-end upgrade**     | Migrated to latest **openai-python v1.x** (`client = OpenAI()`); Whisper transcription with `client.audio.transcriptions.create`.                                   |
| **JSON unwrapping**      | FastAPI strips \`\`\` fences so React receives clean JSON.                                                                                                          |


 Outstanding Tasks / To-Dos
| Priority      | Task                                                                                                                                                                                                                       |
| ------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| ğŸ”´ High       | **A.** Tighten Assistant prompt so every reply **always** includes `"read_aloud"` (40-60 words).<br>**B.** Pass that string from React to `/submit_audio` (already coded) and verify accuracy > 0.                         |
| ğŸŸ  Medium     | 1ï¸âƒ£ Replace `mood.py` stub with real sentiment/facial-expression model (e.g. `mediapipe face_mesh` or OpenAI Vision).<br>2ï¸âƒ£ Decide real reward logic (`reward_token=true` â†’ trigger `utils.comic.generate_reward_comic`). |
| ğŸŸ¡ Low        | â€¢ Persist every `LearnerSnapshot` back into Pinecone (or Postgres).<br>â€¢ Weekly e-mail digests (FastAPI + background task / cron).</br>â€¢ Front-end polish (Tailwind styling, progress bar, disable buttons while loading). |
| ğŸ§‘â€ğŸ”¬ Stretch | â€¢ Auto-embed YouTube link for â€œmultiplication songâ€ (Assistant adds `"media_url"` field).<br>â€¢ Use GPT-4o image generation for one-panel reward comics.                                                                    |


Tech Stack Recap
| Layer             | Choice                                                                                          | Notes                                              |
| ----------------- | ----------------------------------------------------------------------------------------------- | -------------------------------------------------- |
| **Back-end**      | **FastAPI** + **Uvicorn**                                                                       | Async, easy form/multipart handling.               |
| **AI**            | **openai-python â‰¥ 1.13** (`client = OpenAI()`)                                                  | GPT-4o-mini, Whisper-1.                            |
| **Vector DB**     | **Pinecone Serverless (aws us-east-1)**                                                         | Stores learner profile + future snapshots.         |
| **Front-end**     | **React 18 + Vite 6**                                                                           | Hot reload, proxy to FastAPI, Tailwind (optional). |
| **Audio**         | Native `MediaRecorder` (WebM)                                                                   | No extra NPM dependency needed.                    |
| **Webcam**        | `react-webcam`                                                                                  | Simple base64 screenshot.                          |
| **State**         | Pydantic (`LearnerProfile`, `LearnerSnapshot`)                                                  | Held in memory for now.                            |
| **Future extras** | â€¢ Email (SendGrid API) â€¢ Comic generation (`OpenAI.images.generate`) â€¢ Kafka / Redis if scaling |                                                    |


 Next Concrete Steps
    Assistant prompt update: add "read_aloud" to schema; redeploy.

    React UI: show read_aloud; send it as passage with audio.

    Verify accuracy > 0 round-trip.

    Swap in real mood model (or simply random float for now to test pipeline).

    Begin logging snapshots to Pinecone (one upsert per /submit_*).

Once those are in, Karl will get a live adaptive reading + math adventure that records WPM, mood, and progresses automatically.








0 . Quick-start (happy-path)
# clone â†’ create venv â†’ install
git clone <repo> && cd karl_learning_gpt
python -m venv venv && venv\Scripts\activate        # Windows
pip install -r requirements.txt
cp .env.example .env                                 # fill in keys

# back-end (port 8000)
uvicorn app:app --reload --port 8000

# front-end (port 5173)
cd frontend
npm install
npm run dev


Open http://localhost:5173/
â†’ click â€œğŸš€ Start Karlâ€™s Adventureâ€
â†’ activity card with read-aloud block should render.

1 . Folder map (2025-05-27)
karl_learning_gpt/
â”‚   .env.example           â† template for secrets
â”‚   app.py                 â† FastAPI entry
â”‚   prompts.py             â† system prompt (JSON-only)
â”‚   learner_profile.py     â† Pydantic models
â”‚
â”œâ”€ utils/
â”‚   readaloud.py           â† Whisper + fluency
â”‚   mood.py                â† stub (returns 0.0)
â”‚   comic.py               â† stub comic generator
â”‚
â”œâ”€ passages.json           â† legacy fixed snippets
â”œâ”€ store_profile.py        â† Pinecone helper
â”‚
â””â”€ frontend/ (React+Vite)  â† src/App.jsx, AudioRecorder, WebcamSnap, ActivityBox


2 . Environment variables

| Key                             | Example    | Notes                                   |
| ------------------------------- | ---------- | --------------------------------------- |
| `OPENAI_API_KEY`                | sk-â€¦       | used by `OpenAI()` for GPT-4o & Whisper |
| `PINECONE_API_KEY`              | pcsk-â€¦     | vector store                            |
| `PINECONE_ENVIRONMENT`          | `us-east1` | matches console                         |
| *(optional)* `SENDGRID_API_KEY` | SG-â€¦       | weekly email digest                     |


3 . Run / build

| Mode        | Command                                                          |
| ----------- | ---------------------------------------------------------------- |
| Dev â€“ API   | `uvicorn app:app --reload --port 8000`                           |
| Dev â€“ React | `npm run dev` (proxy `/api â†’ 8000`)                              |
| Prod bake   | `npm run build` â†’ `dist/` served by nginx or FastAPI StaticFiles |


4 . FastAPI endpoints (contract)
| Path             | Method         | Body                                   | Returns                          |
| ---------------- | -------------- | -------------------------------------- | -------------------------------- |
| `/start_session` | POST           | â€“                                      | `{thread_id}`                    |
| `/submit_audio`  | POST form-data | `thread_id`, `passage`, `audio` (webm) | WPM, accuracy (%), transcription |
| `/submit_mood`   | POST form-data | `thread_id`, `image` (png)             | `{mood_score}` (float)           |
| `/next_activity` | GET            | `thread_id` (query)                    | raw JSON string from Assistant   |


Assistant JSON schema (v0.2):
{
  "title": "â€¦",
  "story_prompt": "â€¦",
  "steps": ["â€¦", "â€¦"],
  "read_aloud": "40-60 words â€¦",
  "reward_token": false
}


5 . Known gotchas
| Symptom                       | Root cause / Fix                                                              |
| ----------------------------- | ----------------------------------------------------------------------------- |
| `AttributeError: openai.beta` | ensure **openai â‰¥ 1.13** and use `client = OpenAI()`                          |
| `NameError: passage_text`     | must be *inside* `score()`; no globals                                        |
| Accuracy always 0             | passage you diff against â‰  text Karl read â†’ send `"passage"` field from React |
| Activity JSON not parsing     | strip \`\`\` fences (done in app.py) OR tighten Assistant prompt              |
| React button never resets     | call `setRecorder(null)` in `rec.onstop`                                      |

6 . Road-map / backlog
| Priority | Item                                                                | Owner  |
| -------- | ------------------------------------------------------------------- | ------ |
| ğŸ”´       | Replace `mood.py` with real CV model (`mediapipe` or OpenAI Vision) | *open* |
| ğŸ”´       | Persist every `LearnerSnapshot` â†’ Pinecone/DB                       | *open* |
| ğŸŸ        | Weekly email digest (SendGrid)                                      | *open* |
| ğŸŸ        | Reward comic generator (GPT-4o images) when `"reward_token": true`  | *open* |
| ğŸŸ¡       | Front-end polish: progress bar, disable buttons while loading       | *open* |
| ğŸŸ¡       | Unit tests: pytest for utils + FastAPI TestClient                   | *open* |


7 . Suggested next commit
commit the Option B refactor:

    read_aloud displayed, sent, scored

    clean JSON return in /next_activity

bump requirements.txt:
openai>=1.13
fastapi
uvicorn
python-multipart
python-dotenv
pinecone-client

tag v0.2-alpha.